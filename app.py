import streamlit as st
import pandas as pd
import os
import json
from pathlib import Path

from langchain_openai import ChatOpenAI

# ---------- [1] íŒŒì¼ ê²½ë¡œ ë° ë°ì´í„° ê´€ë¦¬ ----------
DB_PATH = './suam_analysis.json'

# ê¸°ì¡´ ë°ì´í„° ë¡œë”© (ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸)
def load_suam_data():
    if os.path.exists(DB_PATH):
        with open(DB_PATH, encoding='utf-8') as f:
            return json.load(f)
    return []

def save_suam_data(data):
    with open(DB_PATH, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

suam_data = load_suam_data()

# ì—°ê´€ ì´ë¡ /ìš©ì–´/ì‚¬ë¡€ ìƒ˜í”Œ DB
basic_theory = [
    {"Category": "í•©ì¶©", "Concept": "ì²œê°„í•©", "Detail": "ê°‘ê¸°í•©í† , ì„ê²½í•©ê¸ˆ ë“±"},
    {"Category": "ì‹­ì‹ ", "Concept": "ì¬ì„±", "Detail": "ì¬ë¬¼ì˜ ì‹¤ì œì„±/í—ˆìƒ íŒë‹¨ ê¸°ì¤€"}
]
terminology = [
    {"Term": "í•©", "Meaning": "ì„œë¡œ ë‹¤ë¥¸ ì˜¤í–‰/ê°„ì§€ì˜ ê²°í•©", "Category": "í•©ì¶©"},
    {"Term": "ì¶©", "Meaning": "ìƒëŒ€ì  íŒŒê´´/ê¸´ì¥", "Category": "í•©ì¶©"}
]
case_studies = [
    {"Birth Info": "ç”²ä¹™è¾›ç™¸ / å¯…å¯é…‰ä¸‘", "Chart": "ç”²ä¹™è¾›ç™¸å¯…å¯é…‰ä¸‘", "Analysis": "ì¬ë¬¼ ì‹¤ì œ ì‘ë™/í—ˆìƒ í•´ì„", "Result": "ì‹¤ì œ ì‘ë™ ì•½í•¨"}
]

# ---------- [2] LLM ìë™ êµ¬ì¡° í•´ì„ í•¨ìˆ˜ ----------
def suam_prompt_builder(tiangan, dizhi, topic):
    # êµ¬ì¡°ì  ê´€ì  í”„ë¡¬í”„íŠ¸ ìë™ ìƒì„±
    prompt = (
        "[ìˆ˜ì•”ëª…ë¦¬ êµ¬ì¡° í•´ì„] "
        "ì•„ë˜ ëª…ì‹ì„ êµ¬ì¡°ì  ê´€ì (ê¸€ì ê°„ ì‹¤ì œ ì—°ê²°/ì‘ìš©ë ¥, í—ˆìƒ ì—¬ë¶€ ë“±)ìœ¼ë¡œ í•´ì„í•´ì¤˜. "
        "ì‹­ì‹  ì„¤ëª…ì€ ìµœì†Œí™”í•˜ê³ , ì¼ê°„ ì¤‘ì‹¬ì˜ ìƒê·¹, í•©, ì¶©, íšŒ, ì‹¤ì œ í˜„ì‹¤ì—ì„œ ì‘ë™í•  ìˆ˜ ìˆëŠ” ì—°ê²°ë§Œ êµ¬ì²´ì ìœ¼ë¡œ ë¶„ì„í•´ì¤˜. "
        f"íŠ¹íˆ â€˜{topic}â€™ì´ ì‹¤ì œ í˜„ì‹¤ì— ì‘ë™í•˜ëŠ”ì§€, í—ˆìƒì¸ì§€, ì´ìœ ê¹Œì§€ ë¶„ì„í•´ì¤˜. "
        "ê²°ê³¼ëŠ” ë‹¤ìŒ í˜•ì‹ìœ¼ë¡œ:\n"
        "1. êµ¬ì¡° í‘œ ìš”ì•½ (ê°„ëµ í‘œ)\n"
        "2. ì²œê°„ ë¶„ì„\n"
        "3. ì§€ì§€ ë¶„ì„\n"
        "4. í•©/ì¶©/í˜•/íŒŒ ë¶„ì„\n"
        "5. í˜„ì‹¤ ì‘ìš©/í•´ì„\n"
        "\nì˜ˆì‹œ ëª…ì‹:\nì²œê°„: " + tiangan + "\nì§€ì§€: " + dizhi
    )
    return prompt

def suam_llm_analyze(api_key, tiangan, dizhi, topic):
    """
    OpenAI LLMì— í”„ë¡¬í”„íŠ¸ë¥¼ ë³´ë‚´ê³ , ì‘ë‹µì„ 5ê°œ í•„ë“œë¡œ ë¶„í•  ë°˜í™˜
    """
    prompt = suam_prompt_builder(tiangan, dizhi, topic)
    llm = ChatOpenAI(openai_api_key=api_key, temperature=0.2, model="gpt-3.5-turbo")
    response = llm.invoke(prompt)
    text = response.content

    # ëŒ€ëµì ìœ¼ë¡œ 1-5ë¡œ ë¶„ë¦¬(ì‹¤ë¬´ì—ì„  ì¢€ ë” robustí•˜ê²Œ íŒŒì‹± ê°€ëŠ¥)
    import re
    parts = re.split(r'\n?[\d\.]+[\)\.] ', text)
    if len(parts) < 6:
        # ì‹¤íŒ¨ì‹œ ê·¸ëƒ¥ ìˆœì„œëŒ€ë¡œ ì±„ì›€
        table, tiangan_desc, dizhi_desc, hapchung_desc, reality = [""]*5
    else:
        _, table, tiangan_desc, dizhi_desc, hapchung_desc, reality = parts[:6]
    return table.strip(), tiangan_desc.strip(), dizhi_desc.strip(), hapchung_desc.strip(), reality.strip(), prompt

# ---------- [3] Streamlit UI/ê¸°ëŠ¥ êµ¬í˜„ ----------
st.set_page_config(page_title="ìˆ˜ì•”ëª…ë¦¬ êµ¬ì¡° í•´ì„ DB", layout="wide")
st.title("ğŸŒ€ ìˆ˜ì•”ëª…ë¦¬ êµ¬ì¡° í•´ì„ ì‹œìŠ¤í…œ (ìë™/ìˆ˜ë™ ì…ë ¥, ì—°ê´€ ìë£Œ í†µí•©)")

tab1, tab2 = st.tabs(["êµ¬ì¡° í•´ì„ ì…ë ¥/ìë™ë¶„ì„", "ê²€ìƒ‰/ìˆ˜ì •/í†µí•©ê²€ìƒ‰"])

# --- êµ¬ì¡° í•´ì„ ì…ë ¥/ìë™ë¶„ì„ ---
with tab1:
    st.subheader("ìˆ˜ì•”ëª…ë¦¬ êµ¬ì¡° í•´ì„ ì…ë ¥ ë° LLM ìë™ ìƒì„±")

    col1, col2 = st.columns([2,1])
    with col1:
        tiangan = st.text_input("ì²œê°„ (ì˜ˆ: å£¬ ç”² è¾› æˆŠ)", key="in1")
        dizhi = st.text_input("ì§€ì§€ (ì˜ˆ: å­ åˆ é…‰ ç”³)", key="in2")
        topic = st.text_input("ì£¼ì œ/ê´€ì  (ì˜ˆ: ì¬ë¬¼ì˜ í˜„ì‹¤ ì‘ë™ë ¥)", value="ì¬ë¬¼ì˜ í˜„ì‹¤ ì‘ë™ë ¥", key="in3")
        api_key = st.text_input("OpenAI API Key (ìë™ í•´ì„ìš©)", type="password")
    with col2:
        auto_gen = st.button("ğŸ”µ LLM ìë™ êµ¬ì¡° í•´ì„")

    # ì„¸ë¶€ êµ¬ì¡° í•„ë“œ
    table_summary = st.text_area("1. êµ¬ì¡° í‘œ ìš”ì•½", height=60, key="f1")
    tiangan_analysis = st.text_area("2. ì²œê°„ ë¶„ì„", height=80, key="f2")
    dizhi_analysis = st.text_area("3. ì§€ì§€ ë¶„ì„", height=80, key="f3")
    hapchung_analysis = st.text_area("4. í•©/ì¶©/í˜•/íŒŒ ë¶„ì„", height=80, key="f4")
    reality_application = st.text_area("5. í˜„ì‹¤ ì‘ìš©/í•´ì„", height=100, key="f5")
    prompt_field = st.text_area("í”„ë¡¬í”„íŠ¸(ì§ˆë¬¸ ë‚´ìš©)", height=100, key="f6")

    # ìë™ ìƒì„± í´ë¦­ ì‹œ LLM ë¶„ì„ â†’ í•„ë“œ ì±„ì›€
    if auto_gen and api_key and tiangan and dizhi and topic:
        with st.spinner("LLM êµ¬ì¡° í•´ì„ ì¤‘..."):
            t, tian, di, hap, real, prompt_used = suam_llm_analyze(api_key, tiangan, dizhi, topic)
            st.session_state.f1 = t
            st.session_state.f2 = tian
            st.session_state.f3 = di
            st.session_state.f4 = hap
            st.session_state.f5 = real
            st.session_state.f6 = prompt_used
            st.success("ìë™ êµ¬ì¡° í•´ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ë‚´ìš© ìˆ˜ì • í›„ ì €ì¥ ê°€ëŠ¥!")

    # ì €ì¥ (ìˆ˜ì •/ì‹ ê·œ í†µí•©)
    edit_idx = st.session_state.get("edit_idx", None)
    if st.button("ğŸ’¾ êµ¬ì¡° í•´ì„ ì €ì¥"):
        entry = {
            "ëª…ì‹_ì²œê°„": tiangan, "ëª…ì‹_ì§€ì§€": dizhi, "ì£¼ì œ": topic,
            "êµ¬ì¡°_í‘œ": st.session_state.f1,
            "ì²œê°„_ë¶„ì„": st.session_state.f2,
            "ì§€ì§€_ë¶„ì„": st.session_state.f3,
            "í•©ì¶©_ë¶„ì„": st.session_state.f4,
            "í˜„ì‹¤_ì‘ìš©": st.session_state.f5,
            "í”„ë¡¬í”„íŠ¸": st.session_state.f6
        }
        if edit_idx is not None:
            suam_data[edit_idx] = entry
            st.session_state.edit_idx = None
            st.success("ê¸°ì¡´ êµ¬ì¡° í•´ì„ì´ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
        else:
            suam_data.append(entry)
            st.success("êµ¬ì¡° í•´ì„ì´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        save_suam_data(suam_data)

# --- ê²€ìƒ‰/ìˆ˜ì •/í†µí•©ê²€ìƒ‰ ---
with tab2:
    st.subheader("êµ¬ì¡° í•´ì„ ë°ì´í„° ê²€ìƒ‰/ìˆ˜ì •/í†µí•©")
    # ì„¸ë¶€ í•„í„°
    keyword = st.text_input("í‚¤ì›Œë“œ ê²€ìƒ‰ (ëª…ì‹, ì£¼ì œ, ë¶„ì„ ë“±)", key="search1")
    field_sel = st.selectbox("ê²€ìƒ‰ í•„ë“œ", ["ì „ì²´", "ëª…ì‹_ì²œê°„", "ëª…ì‹_ì§€ì§€", "ì£¼ì œ", "ì²œê°„_ë¶„ì„", "ì§€ì§€_ë¶„ì„", "í•©ì¶©_ë¶„ì„", "í˜„ì‹¤_ì‘ìš©"])
    search_btn = st.button("ğŸ” ê²€ìƒ‰")
    edit_idx = st.session_state.get("edit_idx", None)

    # ê²€ìƒ‰ ì‹¤í–‰
    if search_btn:
        results = []
        for idx, item in enumerate(suam_data):
            # í•„ë“œ ì„ íƒë³„ ê²€ìƒ‰
            target_text = ""
            if field_sel == "ì „ì²´":
                target_text = " ".join([str(v) for v in item.values()])
            else:
                target_text = item.get(field_sel, "")
            if keyword in target_text:
                results.append((idx, item))
        if results:
            st.info(f"{len(results)}ê±´ ê²€ìƒ‰ë¨ (í´ë¦­ì‹œ í¸ì§‘)")
            for idx, item in results:
                st.markdown(f"---\n#### {idx+1}. [ëª…ì‹: {item['ëª…ì‹_ì²œê°„']} / {item['ëª…ì‹_ì§€ì§€']}] | {item['ì£¼ì œ']}")
                st.markdown(f"- **[1. êµ¬ì¡°í‘œ]**\n{item['êµ¬ì¡°_í‘œ']}")
                st.markdown(f"- **[2. ì²œê°„]**\n{item['ì²œê°„_ë¶„ì„']}")
                st.markdown(f"- **[3. ì§€ì§€]**\n{item['ì§€ì§€_ë¶„ì„']}")
                st.markdown(f"- **[4. í•©ì¶©]**\n{item['í•©ì¶©_ë¶„ì„']}")
                st.markdown(f"- **[5. í˜„ì‹¤ì‘ìš©]**\n{item['í˜„ì‹¤_ì‘ìš©']}")
                st.markdown(f"**[í”„ë¡¬í”„íŠ¸]**\n{item['í”„ë¡¬í”„íŠ¸']}")
                if st.button("ì´ í•´ì„ ë¶ˆëŸ¬ì™€ í¸ì§‘", key=f"edit_{idx}"):
                    # í•´ë‹¹ ë°ì´í„°ë¡œ ì…ë ¥í¼ ì„¸íŒ…
                    st.session_state.in1 = item["ëª…ì‹_ì²œê°„"]
                    st.session_state.in2 = item["ëª…ì‹_ì§€ì§€"]
                    st.session_state.in3 = item["ì£¼ì œ"]
                    st.session_state.f1 = item["êµ¬ì¡°_í‘œ"]
                    st.session_state.f2 = item["ì²œê°„_ë¶„ì„"]
                    st.session_state.f3 = item["ì§€ì§€_ë¶„ì„"]
                    st.session_state.f4 = item["í•©ì¶©_ë¶„ì„"]
                    st.session_state.f5 = item["í˜„ì‹¤_ì‘ìš©"]
                    st.session_state.f6 = item["í”„ë¡¬í”„íŠ¸"]
                    st.session_state.edit_idx = idx
                    st.success("ì…ë ¥ í¼ì— ë°ì´í„°ê°€ ë¶ˆëŸ¬ì™€ì¡ŒìŠµë‹ˆë‹¤. (ìƒë‹¨ íƒ­ì—ì„œ ìˆ˜ì •/ì €ì¥)")
        else:
            st.info("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

    # ì „ì²´ ë°ì´í„° í‘œë¡œ ë³´ê¸°
    st.markdown("---\n#### ì „ì²´ êµ¬ì¡° í•´ì„ ë°ì´í„°")
    df = pd.DataFrame(suam_data)
    if not df.empty:
        st.dataframe(df)

    # ì—°ê´€ ìë£Œ í†µí•© í‘œì¶œ
    st.markdown("---\n### ì—°ê´€ ì´ë¡ /ìš©ì–´/ì‚¬ë¡€ ìë™ ì—°ê²°")
    if search_btn and results:
        st.markdown("#### ì—°ê´€ ì´ë¡ /ìš©ì–´/ì‚¬ë¡€")
        # ì˜ˆì‹œ: ì£¼ì œ, ë¶„ì„ í‚¤ì›Œë“œë¡œ ê´€ë ¨ ì´ë¡ /ìš©ì–´/ì‚¬ë¡€ ìë™ ë§¤ì¹­(ê°„ë‹¨í•˜ê²Œ í¬í•¨ ë‹¨ì–´ ê²€ìƒ‰)
        rel_keywords = set()
        for idx, item in results:
            rel_keywords |= set(item["ì£¼ì œ"].split())
            rel_keywords |= set(item["í•©ì¶©_ë¶„ì„"].split())
            rel_keywords |= set(item["ì²œê°„_ë¶„ì„"].split())
            rel_keywords |= set(item["ì§€ì§€_ë¶„ì„"].split())
        theory_hits = [x for x in basic_theory if any(k in x["Concept"] or k in x["Category"] or k in x["Detail"] for k in rel_keywords)]
        term_hits = [x for x in terminology if any(k in x["Term"] or k in x["Meaning"] for k in rel_keywords)]
        case_hits = [x for x in case_studies if any(k in x["Analysis"] or k in x["Result"] for k in rel_keywords)]
        if theory_hits:
            st.markdown("**[ì—°ê´€ ì´ë¡ ]**")
            st.dataframe(pd.DataFrame(theory_hits))
        if term_hits:
            st.markdown("**[ì—°ê´€ ìš©ì–´]**")
            st.dataframe(pd.DataFrame(term_hits))
        if case_hits:
            st.markdown("**[ì—°ê´€ ì‚¬ë¡€]**")
            st.dataframe(pd.DataFrame(case_hits))
        if not (theory_hits or term_hits or case_hits):
            st.info("ì—°ê´€ ìë£Œê°€ ì—†ìŠµë‹ˆë‹¤.")

st.caption("ë°ì´í„°ëŠ” JSON íŒŒì¼ë¡œ ì €ì¥/ë¶ˆëŸ¬ì˜¤ê¸°ë©ë‹ˆë‹¤. LLM ìë™ í•´ì„, ì…ë ¥ ì„¸ë¶„í™”, ì—°ê´€ìë£Œ, ìˆ˜ì •/ì¬ì €ì¥ê¹Œì§€ í†µí•© ì§€ì›.")